CleanFileHarvest
================

## Features
- Maximum crawling depth, pages
- Repects robots.txt

The overall dataflow for the crawling processing looks like:

![Alt text](architecture.png "Architecture")

## Requirements
- Python v2.7+
- [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/)
- [Robot Exclusion Rules](http://nikitathespider.com/python/rerp/)

## Usage

```
python crawler.py
```

## Reference

The building of this crawler follows instructions from [CS101 in Udacity](https://www.udacity.com/course/viewer#!/c-cs101) and [DaveDaveFind](http://davedavefind.appspot.com/).

